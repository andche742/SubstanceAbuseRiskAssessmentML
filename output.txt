Fitting 5 folds for each of 24 candidates, totalling 120 fits

 Unbalanced results for Alcohol
              precision    recall  f1-score   support

           0       0.00      0.00      0.00        14
           1       0.00      0.00      0.00        48
           2       0.84      1.00      0.91       314

    accuracy                           0.84       376
   macro avg       0.28      0.33      0.30       376
weighted avg       0.70      0.84      0.76       376

/Users/andrewchen/Desktop/SubstanceAbuseRiskAssessmentML/.venv/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
/Users/andrewchen/Desktop/SubstanceAbuseRiskAssessmentML/.venv/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
/Users/andrewchen/Desktop/SubstanceAbuseRiskAssessmentML/.venv/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

 Balanced results for Alcohol
              precision    recall  f1-score   support

           0       0.13      0.21      0.16        14
           1       0.15      0.25      0.19        48
           2       0.85      0.75      0.79       314

    accuracy                           0.66       376
   macro avg       0.38      0.40      0.38       376
weighted avg       0.74      0.66      0.69       376

Fitting 5 folds for each of 24 candidates, totalling 120 fits

 Unbalanced results for Amphet
              precision    recall  f1-score   support

           0       0.72      0.88      0.79       236
           1       0.35      0.36      0.36        85
           2       0.00      0.00      0.00        55

    accuracy                           0.63       376
   macro avg       0.36      0.41      0.38       376
weighted avg       0.53      0.63      0.58       376


 Balanced results for Amphet
              precision    recall  f1-score   support

           0       0.83      0.66      0.73       236
           1       0.39      0.42      0.40        85
           2       0.35      0.62      0.45        55

    accuracy                           0.60       376
   macro avg       0.52      0.57      0.53       376
weighted avg       0.66      0.60      0.62       376

Fitting 5 folds for each of 24 candidates, totalling 120 fits

 Unbalanced results for Amyl
              precision    recall  f1-score   support

           0       0.79      1.00      0.88       297
           1       0.00      0.00      0.00        67
           2       0.00      0.00      0.00        12

    accuracy                           0.79       376
   macro avg       0.26      0.33      0.29       376
weighted avg       0.62      0.79      0.70       376

/Users/andrewchen/Desktop/SubstanceAbuseRiskAssessmentML/.venv/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
/Users/andrewchen/Desktop/SubstanceAbuseRiskAssessmentML/.venv/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
/Users/andrewchen/Desktop/SubstanceAbuseRiskAssessmentML/.venv/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

 Balanced results for Amyl
              precision    recall  f1-score   support

           0       0.88      0.75      0.81       297
           1       0.41      0.51      0.45        67
           2       0.05      0.17      0.08        12

    accuracy                           0.69       376
   macro avg       0.45      0.48      0.45       376
weighted avg       0.77      0.69      0.72       376

Fitting 5 folds for each of 24 candidates, totalling 120 fits

 Unbalanced results for Benzos
              precision    recall  f1-score   support

           0       0.70      0.89      0.78       215
           1       0.35      0.31      0.32        95
           2       0.50      0.12      0.20        66

    accuracy                           0.61       376
   macro avg       0.51      0.44      0.43       376
weighted avg       0.57      0.61      0.56       376


 Balanced results for Benzos
              precision    recall  f1-score   support

           0       0.77      0.71      0.74       215
           1       0.37      0.31      0.33        95
           2       0.42      0.62      0.50        66

    accuracy                           0.59       376
   macro avg       0.52      0.55      0.52       376
weighted avg       0.61      0.59      0.59       376

Fitting 5 folds for each of 24 candidates, totalling 120 fits

 Unbalanced results for Caff
              precision    recall  f1-score   support

           0       0.00      0.00      0.00        10
           1       0.00      0.00      0.00        22
           2       0.91      1.00      0.96       344

    accuracy                           0.91       376
   macro avg       0.30      0.33      0.32       376
weighted avg       0.84      0.91      0.87       376

/Users/andrewchen/Desktop/SubstanceAbuseRiskAssessmentML/.venv/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
/Users/andrewchen/Desktop/SubstanceAbuseRiskAssessmentML/.venv/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
/Users/andrewchen/Desktop/SubstanceAbuseRiskAssessmentML/.venv/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

 Balanced results for Caff
              precision    recall  f1-score   support

           0       0.06      0.20      0.09        10
           1       0.06      0.14      0.08        22
           2       0.92      0.77      0.84       344

    accuracy                           0.72       376
   macro avg       0.34      0.37      0.33       376
weighted avg       0.84      0.72      0.77       376

Fitting 5 folds for each of 24 candidates, totalling 120 fits

 Unbalanced results for Cannabis
              precision    recall  f1-score   support

           0       0.63      0.80      0.70       111
           1       0.51      0.20      0.29        99
           2       0.74      0.87      0.80       166

    accuracy                           0.68       376
   macro avg       0.63      0.63      0.60       376
weighted avg       0.65      0.68      0.64       376


 Balanced results for Cannabis
              precision    recall  f1-score   support

           0       0.61      0.72      0.66       111
           1       0.40      0.37      0.39        99
           2       0.77      0.71      0.74       166

    accuracy                           0.62       376
   macro avg       0.59      0.60      0.60       376
weighted avg       0.63      0.62      0.62       376

Fitting 5 folds for each of 24 candidates, totalling 120 fits

 Unbalanced results for Choc
              precision    recall  f1-score   support

           0       0.00      0.00      0.00         9
           1       0.00      0.00      0.00        24
           2       0.91      1.00      0.95       343

    accuracy                           0.91       376
   macro avg       0.30      0.33      0.32       376
weighted avg       0.83      0.91      0.87       376

/Users/andrewchen/Desktop/SubstanceAbuseRiskAssessmentML/.venv/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
/Users/andrewchen/Desktop/SubstanceAbuseRiskAssessmentML/.venv/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
/Users/andrewchen/Desktop/SubstanceAbuseRiskAssessmentML/.venv/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

 Balanced results for Choc
              precision    recall  f1-score   support

           0       0.00      0.00      0.00         9
           1       0.09      0.12      0.10        24
           2       0.92      0.86      0.89       343

    accuracy                           0.80       376
   macro avg       0.33      0.33      0.33       376
weighted avg       0.84      0.80      0.82       376

Fitting 5 folds for each of 24 candidates, totalling 120 fits

 Unbalanced results for Coke
              precision    recall  f1-score   support

           0       0.67      0.92      0.77       230
           1       0.55      0.29      0.38       112
           2       0.00      0.00      0.00        34

    accuracy                           0.65       376
   macro avg       0.41      0.40      0.39       376
weighted avg       0.57      0.65      0.59       376

/Users/andrewchen/Desktop/SubstanceAbuseRiskAssessmentML/.venv/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
/Users/andrewchen/Desktop/SubstanceAbuseRiskAssessmentML/.venv/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
/Users/andrewchen/Desktop/SubstanceAbuseRiskAssessmentML/.venv/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

 Balanced results for Coke
              precision    recall  f1-score   support

           0       0.82      0.65      0.73       230
           1       0.51      0.33      0.40       112
           2       0.15      0.53      0.23        34

    accuracy                           0.55       376
   macro avg       0.49      0.50      0.45       376
weighted avg       0.67      0.55      0.59       376

Fitting 5 folds for each of 24 candidates, totalling 120 fits

 Unbalanced results for Crack
              precision    recall  f1-score   support

           0       0.89      1.00      0.94       334
           1       0.00      0.00      0.00        40
           2       0.00      0.00      0.00         2

    accuracy                           0.89       376
   macro avg       0.30      0.33      0.31       376
weighted avg       0.79      0.89      0.84       376

/Users/andrewchen/Desktop/SubstanceAbuseRiskAssessmentML/.venv/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
/Users/andrewchen/Desktop/SubstanceAbuseRiskAssessmentML/.venv/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
/Users/andrewchen/Desktop/SubstanceAbuseRiskAssessmentML/.venv/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

 Balanced results for Crack
              precision    recall  f1-score   support

           0       0.94      0.81      0.87       334
           1       0.32      0.60      0.42        40
           2       0.00      0.00      0.00         2

    accuracy                           0.78       376
   macro avg       0.42      0.47      0.43       376
weighted avg       0.87      0.78      0.82       376

Fitting 5 folds for each of 24 candidates, totalling 120 fits

 Unbalanced results for Ecstasy
              precision    recall  f1-score   support

           0       0.77      0.85      0.81       219
           1       0.46      0.58      0.51       106
           2       0.00      0.00      0.00        51

    accuracy                           0.66       376
   macro avg       0.41      0.48      0.44       376
weighted avg       0.58      0.66      0.62       376

/Users/andrewchen/Desktop/SubstanceAbuseRiskAssessmentML/.venv/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
/Users/andrewchen/Desktop/SubstanceAbuseRiskAssessmentML/.venv/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
/Users/andrewchen/Desktop/SubstanceAbuseRiskAssessmentML/.venv/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

 Balanced results for Ecstasy
              precision    recall  f1-score   support

           0       0.82      0.58      0.68       219
           1       0.38      0.47      0.42       106
           2       0.24      0.41      0.31        51

    accuracy                           0.53       376
   macro avg       0.48      0.49      0.47       376
weighted avg       0.61      0.53      0.56       376

Fitting 5 folds for each of 24 candidates, totalling 120 fits

 Unbalanced results for Heroin
              precision    recall  f1-score   support

           0       0.86      1.00      0.93       324
           1       0.00      0.00      0.00        38
           2       0.00      0.00      0.00        14

    accuracy                           0.86       376
   macro avg       0.29      0.33      0.31       376
weighted avg       0.74      0.86      0.80       376

/Users/andrewchen/Desktop/SubstanceAbuseRiskAssessmentML/.venv/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
/Users/andrewchen/Desktop/SubstanceAbuseRiskAssessmentML/.venv/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
/Users/andrewchen/Desktop/SubstanceAbuseRiskAssessmentML/.venv/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

 Balanced results for Heroin
              precision    recall  f1-score   support

           0       0.93      0.78      0.85       324
           1       0.20      0.34      0.25        38
           2       0.11      0.29      0.15        14

    accuracy                           0.72       376
   macro avg       0.41      0.47      0.42       376
weighted avg       0.83      0.72      0.76       376

Fitting 5 folds for each of 24 candidates, totalling 120 fits

 Unbalanced results for Ketamine
              precision    recall  f1-score   support

           0       0.79      1.00      0.88       298
           1       0.00      0.00      0.00        59
           2       0.00      0.00      0.00        19

    accuracy                           0.79       376
   macro avg       0.26      0.33      0.29       376
weighted avg       0.63      0.79      0.70       376

/Users/andrewchen/Desktop/SubstanceAbuseRiskAssessmentML/.venv/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
/Users/andrewchen/Desktop/SubstanceAbuseRiskAssessmentML/.venv/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
/Users/andrewchen/Desktop/SubstanceAbuseRiskAssessmentML/.venv/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

 Balanced results for Ketamine
              precision    recall  f1-score   support

           0       0.89      0.69      0.78       298
           1       0.27      0.49      0.35        59
           2       0.11      0.21      0.15        19

    accuracy                           0.63       376
   macro avg       0.42      0.46      0.42       376
weighted avg       0.75      0.63      0.68       376

Fitting 5 folds for each of 24 candidates, totalling 120 fits

 Unbalanced results for Legalh
              precision    recall  f1-score   support

           0       0.69      0.95      0.80       219
           1       0.54      0.35      0.42       109
           2       0.00      0.00      0.00        48

    accuracy                           0.66       376
   macro avg       0.41      0.43      0.41       376
weighted avg       0.55      0.66      0.59       376

/Users/andrewchen/Desktop/SubstanceAbuseRiskAssessmentML/.venv/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
/Users/andrewchen/Desktop/SubstanceAbuseRiskAssessmentML/.venv/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
/Users/andrewchen/Desktop/SubstanceAbuseRiskAssessmentML/.venv/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

 Balanced results for Legalh
              precision    recall  f1-score   support

           0       0.87      0.70      0.78       219
           1       0.49      0.42      0.46       109
           2       0.32      0.73      0.45        48

    accuracy                           0.62       376
   macro avg       0.56      0.62      0.56       376
weighted avg       0.69      0.62      0.64       376

Fitting 5 folds for each of 24 candidates, totalling 120 fits

 Unbalanced results for LSD
              precision    recall  f1-score   support

           0       0.80      0.90      0.85       250
           1       0.46      0.49      0.48        89
           2       0.00      0.00      0.00        37

    accuracy                           0.72       376
   macro avg       0.42      0.47      0.44       376
weighted avg       0.64      0.72      0.68       376

/Users/andrewchen/Desktop/SubstanceAbuseRiskAssessmentML/.venv/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
/Users/andrewchen/Desktop/SubstanceAbuseRiskAssessmentML/.venv/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
/Users/andrewchen/Desktop/SubstanceAbuseRiskAssessmentML/.venv/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

 Balanced results for LSD
              precision    recall  f1-score   support

           0       0.85      0.81      0.83       250
           1       0.39      0.31      0.35        89
           2       0.26      0.49      0.34        37

    accuracy                           0.66       376
   macro avg       0.50      0.54      0.51       376
weighted avg       0.69      0.66      0.67       376

Fitting 5 folds for each of 24 candidates, totalling 120 fits

 Unbalanced results for Meth
              precision    recall  f1-score   support

           0       0.77      1.00      0.87       291
           1       0.00      0.00      0.00        53
           2       0.00      0.00      0.00        32

    accuracy                           0.77       376
   macro avg       0.26      0.33      0.29       376
weighted avg       0.60      0.77      0.68       376

/Users/andrewchen/Desktop/SubstanceAbuseRiskAssessmentML/.venv/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
/Users/andrewchen/Desktop/SubstanceAbuseRiskAssessmentML/.venv/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
/Users/andrewchen/Desktop/SubstanceAbuseRiskAssessmentML/.venv/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

 Balanced results for Meth
              precision    recall  f1-score   support

           0       0.89      0.76      0.82       291
           1       0.29      0.34      0.31        53
           2       0.22      0.44      0.29        32

    accuracy                           0.67       376
   macro avg       0.46      0.51      0.47       376
weighted avg       0.75      0.67      0.70       376

Fitting 5 folds for each of 24 candidates, totalling 120 fits

 Unbalanced results for Mushrooms
              precision    recall  f1-score   support

           0       0.77      0.85      0.81       226
           1       0.55      0.59      0.57       119
           2       0.00      0.00      0.00        31

    accuracy                           0.70       376
   macro avg       0.44      0.48      0.46       376
weighted avg       0.64      0.70      0.67       376


 Balanced results for Mushrooms
              precision    recall  f1-score   support

           0       0.82      0.73      0.77       226
           1       0.44      0.29      0.35       119
           2       0.17      0.55      0.26        31

    accuracy                           0.57       376
   macro avg       0.48      0.52      0.46       376
weighted avg       0.65      0.57      0.59       376

/Users/andrewchen/Desktop/SubstanceAbuseRiskAssessmentML/.venv/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
/Users/andrewchen/Desktop/SubstanceAbuseRiskAssessmentML/.venv/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
/Users/andrewchen/Desktop/SubstanceAbuseRiskAssessmentML/.venv/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
Fitting 5 folds for each of 24 candidates, totalling 120 fits

 Unbalanced results for Nicotine
              precision    recall  f1-score   support

           0       0.48      0.56      0.52       120
           1       0.00      0.00      0.00        85
           2       0.57      0.78      0.66       171

    accuracy                           0.53       376
   macro avg       0.35      0.45      0.39       376
weighted avg       0.41      0.53      0.46       376


 Balanced results for Nicotine
              precision    recall  f1-score   support

           0       0.50      0.63      0.56       120
           1       0.25      0.24      0.24        85
           2       0.68      0.58      0.62       171

    accuracy                           0.52       376
   macro avg       0.48      0.48      0.48       376
weighted avg       0.53      0.52      0.52       376

Fitting 5 folds for each of 24 candidates, totalling 120 fits
/Users/andrewchen/Desktop/SubstanceAbuseRiskAssessmentML/.venv/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
/Users/andrewchen/Desktop/SubstanceAbuseRiskAssessmentML/.venv/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
/Users/andrewchen/Desktop/SubstanceAbuseRiskAssessmentML/.venv/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

 Unbalanced results for VSA
              precision    recall  f1-score   support

           0       0.87      1.00      0.93       328
           1       0.00      0.00      0.00        40
           2       0.00      0.00      0.00         8

    accuracy                           0.87       376
   macro avg       0.29      0.33      0.31       376
weighted avg       0.76      0.87      0.81       376

/Users/andrewchen/Desktop/SubstanceAbuseRiskAssessmentML/.venv/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
/Users/andrewchen/Desktop/SubstanceAbuseRiskAssessmentML/.venv/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
/Users/andrewchen/Desktop/SubstanceAbuseRiskAssessmentML/.venv/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

 Balanced results for VSA
              precision    recall  f1-score   support

           0       0.93      0.80      0.86       328
           1       0.24      0.50      0.32        40
           2       0.11      0.12      0.12         8

    accuracy                           0.76       376
   macro avg       0.43      0.48      0.43       376
weighted avg       0.84      0.76      0.79       376
